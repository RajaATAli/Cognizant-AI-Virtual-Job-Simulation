{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jpsxhP9Dr5Af"
   },
   "source": [
    "# Task 1 - Exploratory Data Analysis #\n",
    "\n",
    "This notebook will walk you through this task interactively, meaning that once you've imported this notebook into `your local machine` or `jupyter notebook`, you'll be able to run individual cells of code independantly, and see the results as you go. \n",
    "\n",
    "This notebooks is designed for users that have an understanding of **Python** and **data analysis**.\n",
    " \n",
    "---\n",
    "## Section 1 - Setup ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rgaQP4eos9jS"
   },
   "source": [
    "In order to view, analyse and manipulate the dataset `(sample_sales_data.csv)`, we must load it into something called a `dataframe`, which is a way of storing tabulated data in a virtual table. This dataframe will allow us to analyse the data freely. To load it into a dataframe, we will need a package called `Pandas`. We can install pandas with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5x18BTjqy3o",
    "outputId": "dba929c8-ffeb-4ded-b023-6d7a05f6a080"
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur2OdJMttaGP"
   },
   "source": [
    "And now we can import this package like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SP0zwPYq-ef"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M5nmOA2Rtd2E"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 2 - Data loading\n",
    "\n",
    "Now store the CSV file anywhere in your Directory and update the `path` variable below to access it within this notebook. Once we've updated the `path`, let's read this CSV file into a pandas dataframe and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "oDB-Ylm3q_wk",
    "outputId": "b2ae17b8-120e-437c-c3d4-990b6334e0e6"
   },
   "outputs": [],
   "source": [
    "#String named path that represents the location of the CSV file I'm going to load.\n",
    "path = \"/Users/rajaallmdar/Desktop/Cognizant AI Virual Job Experience/Task_1/sample_sales_data.csv\"\n",
    "#Reads the CSV and converts it into a DataFrame.\n",
    "df = pd.read_csv(path)\n",
    "#Drops the first column of the DataFrame (Unnamed: 0).\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True, errors='ignore')\n",
    "#Prints the first 5 rows of the DataFrame.\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI-Q3zvsGWpl"
   },
   "source": [
    "Using the `.head()` method allows us to see the top 5 (5 by default) rows within the dataframe. We can use `.tail()` to see the bottom 5. If you want to see more than 5 rows, simply enter a number into the parentheses, e.g. `head(10)` or `tail(10)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qaXaaIr5Hemv"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 3 - Descriptive statistics\n",
    "\n",
    "In this section, you should try to gain a description of the data, that is: what columns are present, how many null values exist and what data types exists within each column.\n",
    "\n",
    "To get you started an explanation of what the column names mean are provided below:\n",
    "\n",
    "- transaction_id = this is a unique ID that is assigned to each transaction\n",
    "- timestamp = this is the datetime at which the transaction was made\n",
    "- product_id = this is an ID that is assigned to the product that was sold. Each product has a unique ID\n",
    "- category = this is the category that the product is contained within\n",
    "- customer_type = this is the type of customer that made the transaction\n",
    "- unit_price = the price that 1 unit of this item sells for\n",
    "- quantity = the number of units sold for this product within this transaction\n",
    "- total = the total amount payable by the customer\n",
    "- payment_type = the payment method used by the customer\n",
    "\n",
    "After this, you should try to compute some descriptive statistics of the numerical columns within the dataset, such as:\n",
    "\n",
    "- mean\n",
    "- median\n",
    "- count\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbf25LDJ7FJG"
   },
   "outputs": [],
   "source": [
    "print(df.columns) #Displays the name of all columns\n",
    "print(df.isnull().sum()) #Displays the number of null values in each column\n",
    "print(df.dtypes) #Displays the data type of each column\n",
    "\n",
    "\n",
    "#Mean (only of numerical columns)\n",
    "print(df.mean())\n",
    "\n",
    "#Median (only of numerical columns)\n",
    "print(df.median())\n",
    "\n",
    "#Counts the number of non-null values in each column\n",
    "print(df.count())\n",
    "\n",
    "#Getting a count of each unique value\n",
    "print(df['category'].value_counts())\n",
    "print(df['payment_type'].value_counts())\n",
    "\n",
    "#Detailed statistical summary of the entire dataframe\n",
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N-krPtHdHNrh"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 4 - Visualisation using seaborn\n",
    "\n",
    "Now that you've computed some descriptive statistics of the dataset, let's create some visualisations. You may use any package that you wish for visualisation, however, some helper functions have been provided that make use of the `seaborn` package. If you wish to use these helper functions, ensure to run the below cells that install and import `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fT9DrY9RHMrd",
    "outputId": "b792ccca-6123-4bc0-e4f5-8d5888a36450"
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32RDb2C7KOpN"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rNvl5rGeKv-h"
   },
   "source": [
    "To analyse the dataset, below are snippets of code that you can use as helper functions to visualise different columns within the dataset. They include:\n",
    "\n",
    "- plot_continuous_distribution = this is to visualise the distribution of numeric columns\n",
    "- get_unique_values = this is to show how many unique values are present within a column\n",
    "- plot_categorical_distribution = this is to visualise the distribution of categorical columns\n",
    "- correlation_plot = this is to plot the correlations between the numeric columns within the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyUPE4QRKcYQ"
   },
   "outputs": [],
   "source": [
    "#Plots the distribution of continious data\n",
    "#Creates histogram of data in the specified columns and plots a Gaussian Kernel Density Estimate which is a smoothed version of the histogram.\n",
    "def plot_continuous_distribution(data: pd.DataFrame = None, column: str = None, height: int = 8):\n",
    "  _  = sns.displot(data, x=column, kde=True, height=height, aspect=height/5).set(title=f'Distribution of {column}');\n",
    "\n",
    "#prints the number of unique values in a specific column, and the count of each unique values\n",
    "def get_unique_values(data, column):\n",
    "  num_unique_values = len(data[column].unique())\n",
    "  value_counts = data[column].value_counts()\n",
    "  print(f\"Column: {column} has {num_unique_values} unique values\\n\")\n",
    "  print(value_counts)\n",
    "\n",
    "#used to plot the distribution of categorical data.\n",
    "#uses sns.catplot to create a bar chart instead of a histogram\n",
    "def plot_categorical_distribution(data: pd.DataFrame = None, column: str = None, height: int = 8, aspect: int = 2):\n",
    "    catplot = sns.catplot(data=data, x=column, kind='count', height=height, aspect=aspect)\n",
    "    catplot.set(title=f'Distribution of {column}')\n",
    "    catplot.set_xticklabels(rotation=90)  # Rotate labels 90 degrees\n",
    "\n",
    "#Creates a correlation matrix to visualize the correlation between different numerical columns easier\n",
    "def correlation_plot(data: pd.DataFrame = None):\n",
    "  corr = data.corr()\n",
    "  styled = corr.style.background_gradient(cmap='coolwarm')\n",
    "  return styled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Atm2hxjv70Wr"
   },
   "source": [
    "Now it is your chance to visualise the columns, give it your best shot! As well as simply visualising the columns, try to interpret what the results mean in the context of the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the distribution of 'unit_price'\n",
    "plot_continuous_distribution(df, 'unit_price')\n",
    "\n",
    "#Visualize the distribution of 'quantity'\n",
    "plot_continuous_distribution(df, 'quantity')\n",
    "\n",
    "#Get unique values in 'payment_type'\n",
    "get_unique_values(df, 'payment_type')\n",
    "\n",
    "#Visualize the distribution of 'category'\n",
    "plot_categorical_distribution(df, 'category')\n",
    "\n",
    "#Plot correlation between the numerical columns\n",
    "correlation_plot(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oBin5kdG4iS0"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 5 - Summary\n",
    "\n",
    "We have completed an initial exploratory data analysis on the sample of data provided. We should now have a solid understanding of the data. \n",
    "\n",
    "The client wants to know\n",
    "\n",
    "```\n",
    "\"How to better stock the items that they sell\"\n",
    "```\n",
    "\n",
    "From this dataset, it is impossible to answer that question. In order to make the next step on this project with the client, it is clear that:\n",
    "\n",
    "- We need more rows of data. The current sample is only from 1 store and 1 week worth of data\n",
    "- We need to frame the specific problem statement that we want to solve. The current business problem is too broad, we should narrow down the focus in order to deliver a valuable end product\n",
    "- We need more features. Based on the problem statement that we move forward with, we need more columns (features) that may help us to understand the outcome that we're solving for\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "eda",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
